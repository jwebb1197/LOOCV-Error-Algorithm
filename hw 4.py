# -*- coding: utf-8 -*-
"""
LEAVE ONE OUT CROSS VALIDATAION OVER POLYNOMIALS OF DEGREE 1,2,3,4

Created on Tue Apr  7 13:58:12 2020
@author: jwebb
"""

#Somehting to note, LOOCV can be costly on large data sets, so k-fold would be better if working with larger dataset

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures #create design matrix
from sklearn.model_selection import LeaveOneOut
#import statsmodels.formula.api as smf

"""
algorithm in Python that calculates leave one out cross validation error

the algorithm takes an array of predictor values and an array of response values
(each just one column) and calculates the total leave one out error resulting from 
fitting a 1st 2nd 3rd and 4th degree polynomial

The idea: you want to see how well different types of polynomials represent your data

Here the error is defined as the sum of squared errors over every iteration of the 
leave one out validation process

the algorithm prints the degree of the model, and its average error, which is just the
total error divided by the number of observations in the original dataset
"""

#Calculates and prints Leave One Out Cross Validation squared errors, that  
#  result from fitting given data with the four polynomial models of degree
#  1 through 4 using least squares.


def LOOCV_errors(x,y):
    """  
The algorithm has an outer and inner for loop
    """

 #x and y are two arrays with one-column, representing predictor and response values
    
#NOTE: squared error here is defined as a (predicted value - actual value)^2,
#    for every type of model, function sums the squared error corresponding to
#    every left out observation 
    
#USED: PolynomialFeatures, LeaveOneOut(), and LinearRegression() from sklearn 
    
    for degree in [1,2,3,4]:
        
        """
        The outer for loop loops over the possible degrees
        
        In the outer for loop, I generate a design matrix for our predictor values
        using PolynomialFeatures() from the sklearn library. Polynomial features
        takes on the degree as an argument, so for example if we were on the 
        second iteration of the for loop our design matrix would have a column
        of 1's , a column of our original x values, then a third column of our 
        squared x values.
        
        """
        #fit_transform from PolynomialFeatures() is used to generate design matrix
        X = PolynomialFeatures(degree).fit_transform(x.reshape(-1,1))
        loo = LeaveOneOut()
        n = loo.get_n_splits(X) #number of splitting iterations in the cross validator
        loocv_error = 0
        
        for train_index, test_index in loo.split(X):
            """
            next i enter the inner for loop, this iterates over every possible 
            
            train and test index value in the leave one out cross validation process
            
            these indices are generated by LeaveOneOut().split() from the sklearn library
            
            """
            #what loo.split() does: Generates indices to split data into training and test set.
            #      note: test index is just one value because you are "leaving one out", train has 99 values
            
            
            """
            In each iteration of this inner loop I make a train and test subset from the 
            design matrix, and a train and test subset from the original response array,
            which was an argument in our function
    
            """
            
            #assemble test array of X values based off test index from loo.split
            #assemble training array of X values based on remaining indices, ie train_index of loo.split
            #repeat for y values
            X_train, X_test = X[train_index], X[test_index] 
            y_train, y_test = y[train_index], y[test_index]
            
            
            
            """
            Then i use LinearRegression() from the sklearn library to fit a model 
            on the training set
            """
            #fit a linear model using training set of the design matrix and the train y values
            lm = LinearRegression().fit(X_train, y_train)
            
            #predict y value corresponding to the x test value using model
            
            """
            model to predicts the y value that corresponds to the x 
            value in the test set
            
            """
            y_pred = lm.predict(X_test)
            
            
            """
            calculate the squared difference between the predicted
            and actual response. this is the leave one out error, and over each iteration 
            of the inner for loop these error terms are added together.
            """
            #compare predicted to actual y values from test set, calculate squared error,
            #   then add to loocv_error value
            loocv_error += (y_pred - y_test)**2 
            
            
        print("Model: " + str(degree) + " LOOCV error:" + str(loocv_error/n))
   
     


"""
GENERATING DATA TO TRY OUT THE FUNCTION:    
"""

#USE: numpy to set random seed, and generate random values from standard normal distribution
#     matplotlib to generate a scatter plot


""""
I tried this funciton out by generating one hundred random predictor values from 
the standard normal distribution using the numpy library, and plugged these in
to a quadratic formula to obtain 100 response values.



when i used these arrays as arguments in the algorithm:
    
-the 2 degree polynomial had the lowest LOOCV error. this is expected, since
 the y values in our original dataset was generated using a 2-degree polynomial

"""
np.random.seed(480)
x = np.random.normal(0,1,100) #predictor
y = x - 2*(x**2) + np.random.normal(0,1,100) #response, last term is error term

# x and y are each 100 x 1 arrays: n = 100, p = 1
# model used to generate the data in equation form : -2x&2 + x + error, error ~N(0,1)

#Create a scatterplot of X against Y
plt.scatter(x,y) #  parabolic shape, implying a possible quadratic relationship between x and y 
plt.ylabel("Y",)
plt.xlabel("X")
plt.title("Y against X")





#Running the function on generated data:
LOOCV_errors(x,y)






"""
RESULTS FROM RUNNING LOOCV_errors() on x,y from above code:

-the 2 degree polynomial had the lowest LOOCV error. this is expected, since
 the y values in our original dataset was generated using a 2-degree polynomial

-the results would be slightly different with a new seed, but the 2 degree polynomial 
   would still have lowest error
   
"""

        


 

